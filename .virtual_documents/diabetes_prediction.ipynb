# Import necessary libraries for data manipulation
import pandas as pd
import numpy as np

# Import necessary libraries for visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Import necessary libraries for preprocessing
from sklearn.model_selection import train_test_split, GridSearchCV

# Import necessary libraries for model training and evaluation
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier, plot_importance
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay

from imblearn.over_sampling import RandomOverSampler
import joblib








filepath = 'my_diabetes_prediction_data.csv'


# Load the Dataset
data = pd.read_csv(filepath)


# Load your dataset
# data = pd.read_csv('your_dataset.csv')  # Change 'your_dataset.csv' to your actual file name

# Get a random sample of 50 rows
sample_data = data.sample(n=50, random_state=42)  # random_state ensures reproducibility

# Save the random sample to a new CSV file
sample_data.to_csv('random_sample.csv', index=False)





# Display the first few rows of the dataset to get an overview
data.head()


# Basic information about the dataset: columns, non-null values, data types
data.info()





data.dtypes





# Summary statistics for numerical columns
data.describe()





# Check for any missing values in the dataset
data.isnull().sum()





# Chech for Duplicates
data.duplicated().sum()








# Check the distribution of the target variable (diabetes)
data['diabetes'].value_counts()


# Check the distribution of the target variable (diabetes)
data['diabetes'].value_counts(normalize = True)











# 1. Visualize the Distribution of Numerical Features
# Histograms for age, bmi, HbA1c_level, and blood_glucose_level

# List of numerical features
numerical_features = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']

# Set up the figure and axis
plt.figure(figsize=(12, 10))

# Loop through the numerical features and create a subplot for each
for i, feature in enumerate(numerical_features, 1):
    plt.subplot(2, 2, i)  # Create a 2x2 grid of subplots
    sns.histplot(data[feature], kde=True)
    plt.title(f'Distribution of {feature}')

# Adjust the layout to prevent overlap
plt.tight_layout()
plt.show()






#Check the distribution of the Target variable(Diabetes)
plt.figure(figsize=(6,4))
sns.countplot(x='diabetes', data=data)
plt.title("Distribution of Diabetes Cases")
plt.show()








plt.figure(figsize=(10, 6))
corr_matrix = data[['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']
].corr()
sns.heatmap(corr_matrix, annot = True, cmap = 'coolwarm', linewidths = 0.5)
plt.title('Correlation Matrix')
plt.show()





# Set up the figure and axis for a 2x2 grid of subplots
plt.figure(figsize=(12, 10))
# loop through the numerical features and create a subplot for each
for i, feature in enumerate(numerical_features, 1):  # starts from 1
    plt.subplot(2, 2, i)
    sns.boxplot(x = 'diabetes', y = feature, data = data)
    plt.title(f'Boxplot of {feature} by Diabetes')
plt.show()








sns.pairplot(data[numerical_features + ['diabetes']], hue = 'diabetes')
plt.show()














data.duplicated().sum()


# Drop duplicates
data = data.drop_duplicates()


# Comfirm that duplcates has been dropped.
data.duplicated().sum()








train_ds, test_ds = train_test_split(data, test_size = 0.2, random_state = 42, stratify = data['diabetes'])





# Encode the 'gender' and 'smoking_history' column for the training set
train_encoded = pd.get_dummies(train_ds, columns=['gender', 'smoking_history'], drop_first=True)


# Encode the 'gender' and 'smoking_history' column for the testing set
test_encoded = pd.get_dummies(test_ds, columns=['gender', 'smoking_history'], drop_first=True)

# Align the columns of the test set with the training set
test_encoded = test_encoded.reindex(columns=train_encoded.columns, fill_value=0)


# Separate features and target for training set
X_train = train_encoded.drop('diabetes', axis=1)
y_train = train_encoded['diabetes']

# Separate features and target for test set
X_test = test_encoded.drop('diabetes', axis=1)
y_test = test_encoded['diabetes']





# Initialize the RandomOverSampler
ros = RandomOverSampler(random_state=42)

# Resample the training set
X_resampled, y_resampled = ros.fit_resample(X_train, y_train)

# Convert resampled arrays back to DataFrame for consistency
X_train_resampled = pd.DataFrame(X_resampled, columns=X_train.columns)
y_train_resampled = pd.Series(y_resampled, name='diabetes')





X_train_resampled.head()


X_train_resampled.columns





# Function to cap outliers based on IQR
def cap_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    # Cap values at the lower and upper bounds
    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])
    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])
    return df

# Cap outliers in "bmi" and "age"
data_capped = cap_outliers_iqr(data, 'bmi')
data_capped = cap_outliers_iqr(data_capped, 'age')

print(data_capped.describe())  # Check statistics after capping outliers


train_capped_ds, test_capped_ds = train_test_split(data_capped, test_size = 0.2, random_state = 42, stratify = data['diabetes'])


# Encode the 'gender' and 'smoking_history' column for the training set
train_capped_encoded = pd.get_dummies(train_capped_ds, columns=['gender', 'smoking_history'], drop_first=True)

# Encode the 'gender' and 'smoking_history' column for the training set
test_capped_encoded = pd.get_dummies(test_capped_ds, columns=['gender', 'smoking_history'], drop_first=True)

# Align the columns of the test set with the training set
test_capped_encoded = test_capped_encoded.reindex(columns=train_capped_encoded.columns, fill_value=0)

# Separate features and target for training set
X_capped_train = train_capped_encoded.drop('diabetes', axis=1)
y_capped_train = train_capped_encoded['diabetes']

# Separate features and target for test set
X_capped_test = test_capped_encoded.drop('diabetes', axis=1)
y_capped_test = test_capped_encoded['diabetes']

# Initialize the RandomOverSampler
ros = RandomOverSampler(random_state=42)

# Resample the training set
X_capped_resampled, y_capped_resampled = ros.fit_resample(X_capped_train, y_capped_train)

# Convert resampled arrays back to DataFrame for consistency
X_capped_train_resampled = pd.DataFrame(X_capped_resampled, columns=X_train.columns)
y_capped_train_resampled = pd.Series(y_capped_resampled, name='diabetes')








def make_results(model_name:str, model_object, metric:str):
    '''
    Arguments:
        model_name (string): what you want the model to be called in the output table
        model_object: a fit GridSearchCV object
        metric (string): precision, recall, f1, accuracy, or auc
  
    Returns a pandas df with the F1, recall, precision, accuracy, and auc scores
    for the model with the best mean 'metric' score across all validation folds.  
    '''

    # Create dictionary that maps input metric to actual metric name in GridSearchCV
    metric_dict = {'auc': 'mean_test_roc_auc',
                   'precision': 'mean_test_precision',
                   'recall': 'mean_test_recall',
                   'f1': 'mean_test_f1',
                   'accuracy': 'mean_test_accuracy'
                  }

    # Get all the results from the CV and put them in a df
    cv_results = pd.DataFrame(model_object.cv_results_)

    # Isolate the row of the df with the max(metric) score
    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]

    # Extract Accuracy, precision, recall, and f1 score from that row
    auc = best_estimator_results.mean_test_roc_auc
    f1 = best_estimator_results.mean_test_f1
    recall = best_estimator_results.mean_test_recall
    precision = best_estimator_results.mean_test_precision
    accuracy = best_estimator_results.mean_test_accuracy
  
    # Create table of results
    table = pd.DataFrame()
    table = pd.DataFrame({'model': [model_name],
                          'precision': [precision],
                          'recall': [recall],
                          'F1': [f1],
                          'accuracy': [accuracy],
                          'auc': [auc]
                        })
  
    return table





# Instantiate model
log_reg = LogisticRegression(random_state=42)

# Define hyperparameter for model
# param_grid_lr = {
#     'log_reg__C': [0.1, 1, 10],
# }

param_grid_lr = {
    'C': np.logspace(-4, 4, 10),
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear'],  # 'liblinear' supports 'l1' and 'l2'
    'max_iter': [100, 200, 300],
    'class_weight': [None, 'balanced']
}

# Assign a list of scoring metrics to capture
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

grid_search_lr = GridSearchCV(estimator = log_reg, param_grid = param_grid_lr, cv=5, scoring = scoring, refit = 'f1')


%%time
# grid_search_lr.fit(X_capped_train_resampled, y_capped_train_resampled)


# joblib.dump(grid_search_lr, "grid_search_lr.pkl")


grid_search_lr = joblib.load("grid_search_lr.pkl")


# Get all CV scores
LogisticRegression_Results = make_results('LogisticRegression', grid_search_lr, 'f1')
print(LogisticRegression_Results)





decision_tree = DecisionTreeClassifier(random_state=42)

param_grid_dt = {
        'max_depth': [3, 5, 10],
        'min_samples_split': [2, 5, 10]
    }

# Assign a list of scoring metrics to capture
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

grid_search_dt = GridSearchCV(estimator = decision_tree, param_grid = param_grid_dt, cv=5, scoring = scoring, refit = 'f1')


%%time
# grid_search_dt.fit(X_train_resampled, y_train_resampled)


# joblib.dump(grid_search_dt, "grid_search_dt.pkl")


grid_search_dt = joblib.load("grid_search_dt.pkl")


# Get all CV scores
DecisionTreeClassifier_Results = make_results('DecisionTreeClassifier', grid_search_dt, 'f1')
print(DecisionTreeClassifier_Results)





random_forest = RandomForestClassifier(random_state=42)

param_grid_rf = {
        'n_estimators': [100, 200],
        'max_depth': [3, 5, 10],
        'min_samples_split': [2, 5, 10]
    }

# Assign a list of scoring metrics to capture
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

grid_search_rf = GridSearchCV(estimator = random_forest, param_grid = param_grid_rf, cv=5, scoring = scoring, refit = 'f1')


%%time
# grid_search_rf.fit(X_train_resampled, y_train_resampled)


# joblib.dump(grid_search_rf, "grid_search_rf.pkl")


grid_search_rf = joblib.load("grid_search_rf.pkl")


# Get all CV scores
RandomForestClassifier_Results = make_results('RandomForestClassifier', grid_search_rf, 'f1')
print(RandomForestClassifier_Results)


xgboost = XGBClassifier(objective='binary:logistic', random_state=42) 

param_grid_xgb =  {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 10]
}

# Assign a list of scoring metrics to capture
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

grid_search_xgb = GridSearchCV(estimator = xgboost, param_grid = param_grid_xgb, cv=5, scoring = scoring, refit = 'f1')


%%time
# grid_search_xgb.fit(X_train_resampled, y_train_resampled)


# joblib.dump(grid_search_xgb, "grid_search_xgb.pkl")


grid_search_xgb = joblib.load("grid_search_xgb.pkl")


grid_search_xgb.best_score_


# Get all CV scores
XGBClassifier_Results = make_results('XGBClassifier', grid_search_xgb, 'f1')
print(XGBClassifier_Results)








def get_scores(model_name:str, model, X_test_data, y_test_data):
    '''
    Generate a table of test scores.

    In: 
        model_name (string):  How you want your model to be named in the output table
        model:                A fit GridSearchCV object
        X_test_data:          numpy array of X_test data
        y_test_data:          numpy array of y_test data

    Out: pandas df of precision, recall, f1, accuracy, and AUC scores for your model
    '''

    preds = model.best_estimator_.predict(X_test_data)

    auc = roc_auc_score(y_test_data, preds)
    accuracy = accuracy_score(y_test_data, preds)
    precision = precision_score(y_test_data, preds)
    recall = recall_score(y_test_data, preds)
    f1 = f1_score(y_test_data, preds)

    table = pd.DataFrame({'model': [model_name],
                          'precision': [precision], 
                          'recall': [recall],
                          'f1': [f1],
                          'accuracy': [accuracy],
                          'AUC': [auc]
                         })
  
    return table


# Get predictions on test data
xgboost_test_scores = get_scores("XGBoost test", grid_search_xgb, X_test, y_test)


xgboost_test_scores








# Generate array of values for confusion matrix
preds = grid_search_xgb.best_estimator_.predict(X_test)
cm = confusion_matrix(y_test, preds, labels=grid_search_xgb.classes_)

# Plot confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                             display_labels=grid_search_xgb.classes_)
disp.plot(values_format='');





plot_importance(grid_search_xgb.best_estimator_)



